{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e010d3f0",
   "metadata": {},
   "source": [
    "# Amazon Review Analysis (Summaries, Pros/Cons, Demographics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489ee9b8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e161a2f1-f30d-4d1e-9efe-8e9e3cce9651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import torch\n",
    "import collections \n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import spacy\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281c431",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c531421f-d8bd-45ab-b4fb-c1fe659276a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Reviews.csv', nrows = 10000)\n",
    "data = df[['Text', 'Summary']]\n",
    "data.drop_duplicates(subset=['Text'], inplace=True) # Drop duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3b6f1fe-8699-4a45-9892-653fe73f7d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_mapping = {\"ain't\": \"is not\",\"aint\": \"is not\", \"aren't\": \"are not\",\"arent\": \"are not\",\"can't\": \"cannot\",\"cant\": \"cannot\", \"'cause\": \"because\", \"cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", 'mstake':\"mistake\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\",'wasnt':\"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\", 'youve':\"you have\", 'goin':\"going\", '4ward':\"forward\", \"shant\":\"shall not\",'tat':\"that\", 'u':\"you\", 'v': \"we\",'b4':'before', \"sayin'\":\"saying\"\n",
    "                      }\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def text_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([word_mapping[t] if t in word_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    long_words=[]\n",
    "    \n",
    "    tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:                  #removing short word\n",
    "            long_words.append(i) \n",
    "    text = \" \".join(long_words).strip()\n",
    "    def no_space(word, prev_word):\n",
    "        return word in set(',!\"\";.''?') and prev_word!=\" \"\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char for i, char in enumerate(text)]\n",
    "    text = ''.join(out)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7ab2662-c063-496e-a69d-50ce97ae0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_text'] = data['Text'].apply(text_cleaner)\n",
    "data['cleaned_summary'] = data['Summary'].apply(text_cleaner)\n",
    "# this step is to remove all rows that have a blank summary\n",
    "data[\"cleaned_summary\"].replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19d496ce-d6da-4216-ab69-0f02a333a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_text=200 \n",
    "max_len_summary=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "867517c4-2564-4928-8fac-70c13f123f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = tts(data['cleaned_text'],data['cleaned_summary'],test_size=0.1, shuffle=True, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5856a4b0-2098-49b4-a1a1-2906198ffc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function \n",
    "def tokenize(lines, token='word'):\n",
    "    assert token in ('word', 'char'), 'Unknown token type: ' + token\n",
    "    return [line.split() if token == 'word' else list(line) for line in lines]\n",
    "\n",
    "# pading function\n",
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]  # Truncate\n",
    "    return line + [padding_token] * (num_steps - len(line))  # Pad\n",
    "\n",
    "# the vocabulary class \n",
    "class Vocab:\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        # Flatten a 2D list if needed\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The list of unique tokens\n",
    "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
    "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return self.token_to_idx['<unk>']\n",
    "# tokenize\n",
    "src_tokens = tokenize(x_train)\n",
    "tgt_tokens = tokenize(y_train)\n",
    "# build vocabulary on dataset\n",
    "src_vocab = Vocab(src_tokens, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "tgt_vocab = Vocab(tgt_tokens, reserved_tokens=['<pad>', '<bos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "030e5180-8da1-4c4e-825e-fdd2217c0fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn to add eos and padding and also determine valid length of each data sample\n",
    "def build_array_sum(lines, vocab, num_steps):\n",
    "    lines = [vocab[l] for l in lines]\n",
    "    lines = [l + [vocab['<eos>']] for l in lines]\n",
    "    array = torch.tensor([truncate_pad(l, num_steps, vocab['<pad>']) for l in lines])\n",
    "    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
    "    return array, valid_len\n",
    "\n",
    "src_array, src_valid_len = build_array_sum(src_tokens, src_vocab, max_len_text)\n",
    "tgt_array, tgt_valid_len = build_array_sum(tgt_tokens, tgt_vocab, max_len_summary)\n",
    "data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "135421a4-9371-4d16-95c7-07428b0634a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tensor dataset object \n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    dataset = torch.utils.data.TensorDataset(*data_arrays)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "batch_size = 64\n",
    "data_iter = load_array(data_arrays, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b5d62",
   "metadata": {},
   "source": [
    "## Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa585d10-7a5c-4ebe-9be5-b60784109879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main class \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.w_q = nn.Linear(query_size, num_hiddens, bias=bias)\n",
    "        self.w_k = nn.Linear(key_size, num_hiddens, bias=bias)\n",
    "        self.w_v = nn.Linear(value_size, num_hiddens, bias=bias)\n",
    "        self.w_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        queries = transpose_qkv(self.w_q(queries), self.num_heads)\n",
    "        keys = transpose_qkv(self.w_k(keys), self.num_heads)\n",
    "        values = transpose_qkv(self.w_v(values), self.num_heads)\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats = self.num_heads, dim=0)\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "        return self.w_o(output_concat)\n",
    "\n",
    "# Function to transpose the linearly transformed query key and values \n",
    "def transpose_qkv(X, num_heads):\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "# For output formatting \n",
    "def transpose_output(X, num_heads):\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)\n",
    "\n",
    "# The dot product attention scoring function \n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2))/math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "# Here masking is used so that irrelevant padding tokens are not considered\n",
    "# while calculations\n",
    "\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32)[None, :] < valid_len[:, None]    #device=X.device\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "# the irrelevant tokens are given a very small negative value which gets\n",
    "# ignored in the subsequent calculations\n",
    "def masked_softmax(X, valid_lens):\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1) \n",
    "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45f5a1e5-c401-4b98-bcda-16d606b3367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_output, **kwargs):\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_output)\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "473ab623-7b11-4e94-aeed-48c10072a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1)/torch.pow(10000,torch.arange(0, num_hiddens,2, dtype=torch.float32)/num_hiddens)\n",
    "        self.P[:,:, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cb352f6-e1c4-43a3-8940-6335008aae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, dropout):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(X + self.dropout(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a044a5aa-bf76-41f3-8e46-6f754ceb8bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for the block structure within \n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, \n",
    "                 ffn_num_hiddens, num_heads, dropout, use_bias=False, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.attention = MultiHeadAttention(key_size, query_size, value_size, num_hiddens,num_heads, dropout, use_bias)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "        \n",
    "    def forward(self, X, valid_lens):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
    "        return self.addnorm2(Y, self.ffn(Y))\n",
    "\n",
    "# the main encoder class\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, use_bias=False, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i),EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias))\n",
    "    \n",
    "    def forward(self, X, valid_lens, *args):\n",
    "        X = self.pos_encoding(self.embedding(X)*math.sqrt(self.num_hiddens))\n",
    "        self.attention_weights = [None]*len(self.blks)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, valid_lens)\n",
    "            self.attention_weights[i] = blk.attention.attention.attention_weights\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4fd9c60-31a5-4069-a83a-8fcb920e63c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "                 ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, **kwargs):\n",
    "        super(DecoderBlock, self).__init__(**kwargs)\n",
    "        self.i = i\n",
    "        self.attention1 = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.attention2 = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm3 = AddNorm(norm_shape, dropout)\n",
    "        \n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "        if state[2][self.i] is None: # true when training the model\n",
    "            key_values = X\n",
    "        else:                        # while decoding state[2][self.i] is decoded output of the ith block till the present time-step\n",
    "            key_values = torch.cat((state[2][self.i], X), axis=1)\n",
    "        state[2][self.i] = key_values\n",
    "        if self.training:\n",
    "            batch_size, num_steps, _ = X.shape\n",
    "            dec_valid_lens = torch.arange(1, num_steps+1, device = X.device).repeat(batch_size, 1)\n",
    "        else:\n",
    "            dec_valid_lens = None\n",
    "        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)\n",
    "        Y = self.addnorm1(X, X2)\n",
    "        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "        Z = self.addnorm2(Y, Y2)\n",
    "        return self.addnorm3(Z, self.ffn(Z)), state\n",
    "\n",
    "# The main decoder class \n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i), \n",
    "                                DecoderBlock(key_size, query_size, value_size,\n",
    "                                             num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i))\n",
    "            self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "    \n",
    "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
    "        return [enc_outputs, enc_valid_lens, [None]*self.num_layers]\n",
    "    \n",
    "    def forward(self, X, state):\n",
    "        X = self.pos_encoding(self.embedding(X)*math.sqrt(self.num_hiddens))\n",
    "        self._attention_weights = [[None]*len(self.blks) for _ in range(2)]\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X, state = blk(X, state)\n",
    "            self._attention_weights[0][i] = blk.attention1.attention.attention_weights\n",
    "            self._attention_weights[1][i] = blk.attention2.attention.attention_weights\n",
    "        return self.dense(X), state\n",
    "    \n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "421064ba-59c3-41c8-9672-d7bc7a5a0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_all_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_all_outputs, *args)\n",
    "        # Return decoder output only\n",
    "        return self.decoder(dec_X, dec_state)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d77eeee-7fc0-488d-a324-96a68574c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device(i=0):\n",
    "    if torch.cuda.device_count() >= i+1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4cf2ea4-5beb-4eb7-8ff8-e73b81bddf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(layers):\n",
    "    if type(layers) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(layers.weight)\n",
    "    if (type(layers) == nn.LSTM or type(layers) == nn.GRU):\n",
    "        for param in layers._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(layers._parameters[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ceb8edff-efb0-41f3-81e7-e5ede1634368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embedding): Embedding(16455, 32)\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (blks): Sequential(\n",
       "      (block0): EncoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (addnorm1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): PositionWiseFFN(\n",
       "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
       "        )\n",
       "        (addnorm2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (block1): EncoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (addnorm1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): PositionWiseFFN(\n",
       "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
       "        )\n",
       "        (addnorm2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (embedding): Embedding(4015, 32)\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (blks): Sequential(\n",
       "      (block0): DecoderBlock(\n",
       "        (attention1): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (addnorm1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attention2): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (addnorm2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): PositionWiseFFN(\n",
       "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
       "        )\n",
       "        (addnorm3): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (block1): DecoderBlock(\n",
       "        (attention1): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (addnorm1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attention2): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (addnorm2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): PositionWiseFFN(\n",
       "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
       "        )\n",
       "        (addnorm3): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dense): Linear(in_features=32, out_features=4015, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens, num_layers, dropout, num_steps = 32, 2, 0.1, 10\n",
    "ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4\n",
    "key_size, query_size, value_size = 32, 32, 32\n",
    "norm_shape = [32]\n",
    "encoder = TransformerEncoder(len(src_vocab), key_size, query_size, value_size, num_hiddens,norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,num_layers, dropout)\n",
    "decoder = TransformerDecoder(len(tgt_vocab), key_size, query_size, value_size, num_hiddens,norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,num_layers, dropout)\n",
    "net = Transformer(encoder, decoder)\n",
    "net.apply(initialize_weights)  # initialize the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fecac768-09dc-448a-9ca8-6ed4ca4ddf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bddf9b64-7f1f-4f7c-9f99-628c6ab4a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebaa385b-59cd-42d2-b2e5-30f2f9a31ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
    "    # `label` shape: (`batch_size`, `num_steps`)\n",
    "    # `valid_len` shape: (`batch_size`,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction='none'\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(pred.permute(0, 2, 1), label)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a66e91",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2370c67d-c73a-4bee-bfae-72efdc79bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):    \n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = Accumulator(2)  # Sum of training loss, no. of tokens\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],device=device).reshape(-1, 1)\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # Teacher forcing\n",
    "            Y_hat = net(X, dec_input, X_valid_len)\n",
    "            l = loss(Y_hat, Y, Y_valid_len)\n",
    "            l.sum().backward()  # Make the loss scalar for `backward`\n",
    "            grad_clipping(net, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l.sum(), num_tokens)\n",
    "        print(f\"Done with epoch number: {epoch+1}\") # optional step\n",
    "    print(f'loss {metric[0] / metric[1]:.3f} on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99a17ae5-ab76-4e50-a4da-490a9e91345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "num_epochs = 100\n",
    "\n",
    "# Uncomment the line below to train the model\n",
    "# train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28da64fe-8d8b-41d9-a297-ec574fd77bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,device, save_attention_weights=False):\n",
    "    # Set `net` to eval mode for inference\n",
    "    net.eval()\n",
    "    src_tokens = src_vocab[src_sentence.lower().split()] + [src_vocab['<eos>']]\n",
    "    src_tokens = [src_vocab[token] if token in src_vocab.token_to_idx else src_vocab['<unk>'] for token in src_sentence.lower().split()]\n",
    "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
    "    src_tokens = truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    # Unsqueeze adds another dimension that works as the the batch axis here\n",
    "    enc_X = torch.unsqueeze(torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    # Add the batch axis to the decoder now\n",
    "    dec_X = torch.unsqueeze(torch.tensor([tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    for _ in range(num_steps):\n",
    "        Y = net.decoder(dec_X, dec_state)[0]\n",
    "        # We use the token with the highest prediction likelihood as the input\n",
    "        # of the decoder at the next time step\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # Save attention weights\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights)\n",
    "            # Once the end-of-sequence token is predicted, the generation of the output sequence is complete\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "                break\n",
    "        output_seq.append(pred)\n",
    "    if len(output_seq)<2:\n",
    "        \n",
    "        if len(output_seq)==1: \n",
    "            return ''.join(tgt_vocab.to_tokens(output_seq[0])), attention_weight_seq   \n",
    "        else:\n",
    "            \n",
    "            return \"No output!\", attention_weight_seq\n",
    "    else:\n",
    "        return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf27ee26",
   "metadata": {},
   "source": [
    "## Applying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f16757bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embedding): Embedding(16455, 32)\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (blks): Sequential(\n",
       "      (block0): EncoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (addnorm1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): PositionWiseFFN(\n",
       "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
       "        )\n",
       "        (addnorm2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (block1): EncoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (addnorm1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): PositionWiseFFN(\n",
       "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
       "        )\n",
       "        (addnorm2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (embedding): Embedding(4015, 32)\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (blks): Sequential(\n",
       "      (block0): DecoderBlock(\n",
       "        (attention1): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (addnorm1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attention2): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (addnorm2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): PositionWiseFFN(\n",
       "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
       "        )\n",
       "        (addnorm3): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (block1): DecoderBlock(\n",
       "        (attention1): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (addnorm1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attention2): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (addnorm2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): PositionWiseFFN(\n",
       "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
       "        )\n",
       "        (addnorm3): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dense): Linear(in_features=32, out_features=4015, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Transformer(encoder, decoder)\n",
    "net.load_state_dict(torch.load(\"transformer_summarizer.pth\", map_location=device))\n",
    "net.to(device)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57c1eaa6-252d-401e-9bdf-1cdcf1830473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_product_reviews(product_id, df, net, src_vocab, tgt_vocab, num_steps, device):\n",
    "    # Filter reviews for the given product ID\n",
    "    reviews = df[df['ProductId'] == product_id]['Text'].dropna().tolist()\n",
    "    if not reviews:\n",
    "        return \"No reviews found for this product ID.\"\n",
    "    \n",
    "    # Clean and concatenate all reviews into one string\n",
    "    cleaned_reviews = [text_cleaner(review) for review in reviews]\n",
    "    full_text = ' '.join(cleaned_reviews)\n",
    "    \n",
    "    # Summarize\n",
    "    summary, _ = predict_seq2seq(net, full_text, src_vocab, tgt_vocab, num_steps, device)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27fdef25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This product is popular among both males and females.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Pros:\n",
      "    Pros  Mentions\n",
      "favorite        15\n",
      "  smooth        10\n",
      "    nice         7\n",
      "   yummy         5\n",
      "   light         4\n",
      "\n",
      "Top Cons:\n",
      "     Cons  Mentions\n",
      "     weak         7\n",
      "   bitter         5\n",
      " horrible         3\n",
      "defective         2\n",
      " terrible         2\n",
      "\n",
      "Summary of Reviews:\n",
      "good coffee brews hot\n"
     ]
    }
   ],
   "source": [
    "productId = \"B003VXFK44\" # Set the product ID you want to analyze\n",
    "\n",
    "loaded_model = joblib.load('gender_predictor.pkl')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "pros = {}\n",
    "cons = {}\n",
    "\n",
    "def analyze_review(review, rating):    \n",
    "    # Extract adjectives (possible pros/cons)\n",
    "    doc = nlp(review)\n",
    "    excluded_words = ['good', 'bad', 'great', 'amazing', 'wonderful', 'best', 'many']\n",
    "    aspects = [token.text.lower() for token in doc if token.pos_ == 'ADJ' and token.text.lower() not in excluded_words]\n",
    "\n",
    "    # Positive Review\n",
    "    if rating >= 4:\n",
    "        for aspect in aspects:\n",
    "            pros[aspect] = pros.get(aspect, 0) + 1\n",
    "    # Negative review\n",
    "    elif rating <= 2:\n",
    "        for aspect in aspects:\n",
    "            cons[aspect] = cons.get(aspect, 0) + 1\n",
    "    \n",
    "    return\n",
    "\n",
    "def predict_gender(name):\n",
    "    predicted_gender = loaded_model.predict([name.lower()])\n",
    "    if predicted_gender[0] == 0:\n",
    "        return('Male')\n",
    "    else:\n",
    "        return('Female')\n",
    "\n",
    "# Extract Pros/Cons and Gender from each matching productId\n",
    "df.loc[df['ProductId'] == productId, 'Extracted Aspects'] = df[df['ProductId'] == productId].apply(lambda row: analyze_review(row['Summary'], row['Score']), axis=1)\n",
    "df.loc[df['ProductId'] == productId, 'Gender'] = df[df['ProductId'] == productId].apply(lambda row: predict_gender(row['ProfileName']), axis=1)\n",
    "\n",
    "# Calculate gender proportion\n",
    "num_reviews = len(df[(df['ProductId'] == productId)])\n",
    "num_males = len(df[(df['ProductId'] == productId) & (df['Gender'] == 'Male')])\n",
    "male_proportion = num_males / num_reviews\n",
    "\n",
    "# Create DataFrames for Pros and Cons for visualization\n",
    "top_pros = dict(sorted(pros.items(), key=lambda item: item[1], reverse=True)[:5])  # Top 5 Pros\n",
    "top_cons = dict(sorted(cons.items(), key=lambda item: item[1], reverse=True)[:5])  # Top 5 Cons\n",
    "pros_df = pd.DataFrame(list(top_pros.items()), columns=['Pros', 'Mentions'])\n",
    "cons_df = pd.DataFrame(list(top_cons.items()), columns=['Cons', 'Mentions'])\n",
    "\n",
    "# Output results\n",
    "if male_proportion < 0.4:\n",
    "    print('This product is most popular among females.')\n",
    "elif male_proportion < 0.6:\n",
    "    print('This product is popular among both males and females.')\n",
    "else:\n",
    "    print('This product is most popular among males')\n",
    "\n",
    "# Pie chart data\n",
    "labels = ['Male', 'Female']\n",
    "sizes = [num_males, num_reviews-num_males]\n",
    "colors = ['#1f77b4', '#ff7f0e']  # Blue for Male, Orange for Female\n",
    "\n",
    "# Plotting the pie chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=100)\n",
    "plt.title('Gender Distribution for Product ID {}'.format(productId))\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()\n",
    "\n",
    "print('\\nTop Pros:')\n",
    "print(pros_df.to_string(index=False))\n",
    "\n",
    "print('\\nTop Cons:')\n",
    "print(cons_df.to_string(index=False))\n",
    "\n",
    "print('\\nSummary of Reviews:')\n",
    "print(summarize_product_reviews(productId, df, net, src_vocab, tgt_vocab, max_len_text, device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
