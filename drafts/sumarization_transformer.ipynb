{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e161a2f1-f30d-4d1e-9efe-8e9e3cce9651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import torch\n",
    "import collections \n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ec955a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c531421f-d8bd-45ab-b4fb-c1fe659276a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Reviews.csv', nrows = 10000)\n",
    "data = df[['Text', 'Summary']]\n",
    "data.drop_duplicates(subset=['Text'], inplace=True) # Drop duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3b6f1fe-8699-4a45-9892-653fe73f7d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_mapping = {\"ain't\": \"is not\",\"aint\": \"is not\", \"aren't\": \"are not\",\"arent\": \"are not\",\"can't\": \"cannot\",\"cant\": \"cannot\", \"'cause\": \"because\", \"cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", 'mstake':\"mistake\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\",'wasnt':\"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\", 'youve':\"you have\", 'goin':\"going\", '4ward':\"forward\", \"shant\":\"shall not\",'tat':\"that\", 'u':\"you\", 'v': \"we\",'b4':'before', \"sayin'\":\"saying\"\n",
    "                      }\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def text_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([word_mapping[t] if t in word_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    long_words=[]\n",
    "    \n",
    "    tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:                  #removing short word\n",
    "            long_words.append(i) \n",
    "    text = \" \".join(long_words).strip()\n",
    "    def no_space(word, prev_word):\n",
    "        return word in set(',!\"\";.''?') and prev_word!=\" \"\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char for i, char in enumerate(text)]\n",
    "    text = ''.join(out)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7ab2662-c063-496e-a69d-50ce97ae0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_text'] = data['Text'].apply(text_cleaner)\n",
    "data['cleaned_summary'] = data['Summary'].apply(text_cleaner)\n",
    "# this step is to remove all rows that have a blank summary\n",
    "data[\"cleaned_summary\"].replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19d496ce-d6da-4216-ab69-0f02a333a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_text=200 \n",
    "max_len_summary=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "867517c4-2564-4928-8fac-70c13f123f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = tts(data['cleaned_text'],data['cleaned_summary'],test_size=0.1, shuffle=True, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5856a4b0-2098-49b4-a1a1-2906198ffc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function \n",
    "def tokenize(lines, token='word'):\n",
    "    assert token in ('word', 'char'), 'Unknown token type: ' + token\n",
    "    return [line.split() if token == 'word' else list(line) for line in lines]\n",
    "\n",
    "# pading function\n",
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]  # Truncate\n",
    "    return line + [padding_token] * (num_steps - len(line))  # Pad\n",
    "\n",
    "# the vocabulary class \n",
    "class Vocab:\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        # Flatten a 2D list if needed\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The list of unique tokens\n",
    "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
    "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return self.token_to_idx['<unk>']\n",
    "# tokenize\n",
    "src_tokens = tokenize(x_train)\n",
    "tgt_tokens = tokenize(y_train)\n",
    "# build vocabulary on dataset\n",
    "src_vocab = Vocab(src_tokens, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "tgt_vocab = Vocab(tgt_tokens, reserved_tokens=['<pad>', '<bos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "030e5180-8da1-4c4e-825e-fdd2217c0fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn to add eos and padding and also determine valid length of each data sample\n",
    "def build_array_sum(lines, vocab, num_steps):\n",
    "    lines = [vocab[l] for l in lines]\n",
    "    lines = [l + [vocab['<eos>']] for l in lines]\n",
    "    array = torch.tensor([truncate_pad(l, num_steps, vocab['<pad>']) for l in lines])\n",
    "    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
    "    return array, valid_len\n",
    "\n",
    "src_array, src_valid_len = build_array_sum(src_tokens, src_vocab, max_len_text)\n",
    "tgt_array, tgt_valid_len = build_array_sum(tgt_tokens, tgt_vocab, max_len_summary)\n",
    "data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "135421a4-9371-4d16-95c7-07428b0634a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tensor dataset object \n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    dataset = torch.utils.data.TensorDataset(*data_arrays)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "batch_size = 64\n",
    "data_iter = load_array(data_arrays, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa585d10-7a5c-4ebe-9be5-b60784109879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main class \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.w_q = nn.Linear(query_size, num_hiddens, bias=bias)\n",
    "        self.w_k = nn.Linear(key_size, num_hiddens, bias=bias)\n",
    "        self.w_v = nn.Linear(value_size, num_hiddens, bias=bias)\n",
    "        self.w_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        queries = transpose_qkv(self.w_q(queries), self.num_heads)\n",
    "        keys = transpose_qkv(self.w_k(keys), self.num_heads)\n",
    "        values = transpose_qkv(self.w_v(values), self.num_heads)\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats = self.num_heads, dim=0)\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "        return self.w_o(output_concat)\n",
    "\n",
    "# Function to transpose the linearly transformed query key and values \n",
    "def transpose_qkv(X, num_heads):\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "# For output formatting \n",
    "def transpose_output(X, num_heads):\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)\n",
    "\n",
    "# The dot product attention scoring function \n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2))/math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "# Here masking is used so that irrelevant padding tokens are not considered\n",
    "# while calculations\n",
    "\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32)[None, :] < valid_len[:, None]    #device=X.device\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "# the irrelevant tokens are given a very small negative value which gets\n",
    "# ignored in the subsequent calculations\n",
    "def masked_softmax(X, valid_lens):\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1) \n",
    "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45f5a1e5-c401-4b98-bcda-16d606b3367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_output, **kwargs):\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_output)\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "473ab623-7b11-4e94-aeed-48c10072a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1)/torch.pow(10000,torch.arange(0, num_hiddens,2, dtype=torch.float32)/num_hiddens)\n",
    "        self.P[:,:, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cb352f6-e1c4-43a3-8940-6335008aae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, dropout):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(X + self.dropout(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a044a5aa-bf76-41f3-8e46-6f754ceb8bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for the block structure within \n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, \n",
    "                 ffn_num_hiddens, num_heads, dropout, use_bias=False, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.attention = MultiHeadAttention(key_size, query_size, value_size, num_hiddens,num_heads, dropout, use_bias)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "        \n",
    "    def forward(self, X, valid_lens):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
    "        return self.addnorm2(Y, self.ffn(Y))\n",
    "\n",
    "# the main encoder class\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, use_bias=False, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i),EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias))\n",
    "    \n",
    "    def forward(self, X, valid_lens, *args):\n",
    "        X = self.pos_encoding(self.embedding(X)*math.sqrt(self.num_hiddens))\n",
    "        self.attention_weights = [None]*len(self.blks)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, valid_lens)\n",
    "            self.attention_weights[i] = blk.attention.attention.attention_weights\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4fd9c60-31a5-4069-a83a-8fcb920e63c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "                 ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, **kwargs):\n",
    "        super(DecoderBlock, self).__init__(**kwargs)\n",
    "        self.i = i\n",
    "        self.attention1 = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.attention2 = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm3 = AddNorm(norm_shape, dropout)\n",
    "        \n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "        if state[2][self.i] is None: # true when training the model\n",
    "            key_values = X\n",
    "        else:                        # while decoding state[2][self.i] is decoded output of the ith block till the present time-step\n",
    "            key_values = torch.cat((state[2][self.i], X), axis=1)\n",
    "        state[2][self.i] = key_values\n",
    "        if self.training:\n",
    "            batch_size, num_steps, _ = X.shape\n",
    "            dec_valid_lens = torch.arange(1, num_steps+1, device = X.device).repeat(batch_size, 1)\n",
    "        else:\n",
    "            dec_valid_lens = None\n",
    "        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)\n",
    "        Y = self.addnorm1(X, X2)\n",
    "        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "        Z = self.addnorm2(Y, Y2)\n",
    "        return self.addnorm3(Z, self.ffn(Z)), state\n",
    "\n",
    "# The main decoder class \n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i), \n",
    "                                DecoderBlock(key_size, query_size, value_size,\n",
    "                                             num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i))\n",
    "            self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "    \n",
    "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
    "        return [enc_outputs, enc_valid_lens, [None]*self.num_layers]\n",
    "    \n",
    "    def forward(self, X, state):\n",
    "        X = self.pos_encoding(self.embedding(X)*math.sqrt(self.num_hiddens))\n",
    "        self._attention_weights = [[None]*len(self.blks) for _ in range(2)]\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X, state = blk(X, state)\n",
    "            self._attention_weights[0][i] = blk.attention1.attention.attention_weights\n",
    "            self._attention_weights[1][i] = blk.attention2.attention.attention_weights\n",
    "        return self.dense(X), state\n",
    "    \n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "421064ba-59c3-41c8-9672-d7bc7a5a0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_all_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_all_outputs, *args)\n",
    "        # Return decoder output only\n",
    "        return self.decoder(dec_X, dec_state)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d77eeee-7fc0-488d-a324-96a68574c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device(i=0):\n",
    "    if torch.cuda.device_count() >= i+1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4cf2ea4-5beb-4eb7-8ff8-e73b81bddf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(layers):\n",
    "    if type(layers) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(layers.weight)\n",
    "    if (type(layers) == nn.LSTM or type(layers) == nn.GRU):\n",
    "        for param in layers._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(layers._parameters[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ceb8edff-efb0-41f3-81e7-e5ede1634368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embedding): Embedding(16455, 32)\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (blks): Sequential(\n",
       "      (block0): EncoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (addnorm1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): PositionWiseFFN(\n",
       "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
       "        )\n",
       "        (addnorm2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (block1): EncoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (addnorm1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): PositionWiseFFN(\n",
       "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
       "        )\n",
       "        (addnorm2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (embedding): Embedding(4015, 32)\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (blks): Sequential(\n",
       "      (block0): DecoderBlock(\n",
       "        (attention1): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (addnorm1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attention2): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (addnorm2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): PositionWiseFFN(\n",
       "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
       "        )\n",
       "        (addnorm3): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (block1): DecoderBlock(\n",
       "        (attention1): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (addnorm1): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (attention2): MultiHeadAttention(\n",
       "          (attention): DotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (w_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (w_o): Linear(in_features=32, out_features=32, bias=False)\n",
       "        )\n",
       "        (addnorm2): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): PositionWiseFFN(\n",
       "          (dense1): Linear(in_features=32, out_features=64, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dense2): Linear(in_features=64, out_features=32, bias=True)\n",
       "        )\n",
       "        (addnorm3): AddNorm(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dense): Linear(in_features=32, out_features=4015, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens, num_layers, dropout, num_steps = 32, 2, 0.1, 10\n",
    "ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4\n",
    "key_size, query_size, value_size = 32, 32, 32\n",
    "norm_shape = [32]\n",
    "encoder = TransformerEncoder(len(src_vocab), key_size, query_size, value_size, num_hiddens,norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,num_layers, dropout)\n",
    "decoder = TransformerDecoder(len(tgt_vocab), key_size, query_size, value_size, num_hiddens,norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,num_layers, dropout)\n",
    "net = Transformer(encoder, decoder)\n",
    "net.apply(initialize_weights)  # initialize the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fecac768-09dc-448a-9ca8-6ed4ca4ddf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bddf9b64-7f1f-4f7c-9f99-628c6ab4a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebaa385b-59cd-42d2-b2e5-30f2f9a31ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
    "    # `label` shape: (`batch_size`, `num_steps`)\n",
    "    # `valid_len` shape: (`batch_size`,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction='none'\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(pred.permute(0, 2, 1), label)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2370c67d-c73a-4bee-bfae-72efdc79bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):    \n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = Accumulator(2)  # Sum of training loss, no. of tokens\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],device=device).reshape(-1, 1)\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # Teacher forcing\n",
    "            Y_hat = net(X, dec_input, X_valid_len)\n",
    "            l = loss(Y_hat, Y, Y_valid_len)\n",
    "            l.sum().backward()  # Make the loss scalar for `backward`\n",
    "            grad_clipping(net, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l.sum(), num_tokens)\n",
    "        print(f\"Done with epoch number: {epoch+1}\") # optional step\n",
    "    print(f'loss {metric[0] / metric[1]:.3f} on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99a17ae5-ab76-4e50-a4da-490a9e91345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with epoch number: 1\n",
      "Done with epoch number: 2\n",
      "Done with epoch number: 3\n",
      "Done with epoch number: 4\n",
      "Done with epoch number: 5\n",
      "Done with epoch number: 6\n",
      "Done with epoch number: 7\n",
      "Done with epoch number: 8\n",
      "Done with epoch number: 9\n",
      "Done with epoch number: 10\n",
      "Done with epoch number: 11\n",
      "Done with epoch number: 12\n",
      "Done with epoch number: 13\n",
      "Done with epoch number: 14\n",
      "Done with epoch number: 15\n",
      "Done with epoch number: 16\n",
      "Done with epoch number: 17\n",
      "Done with epoch number: 18\n",
      "Done with epoch number: 19\n",
      "Done with epoch number: 20\n",
      "Done with epoch number: 21\n",
      "Done with epoch number: 22\n",
      "Done with epoch number: 23\n",
      "Done with epoch number: 24\n",
      "Done with epoch number: 25\n",
      "Done with epoch number: 26\n",
      "Done with epoch number: 27\n",
      "Done with epoch number: 28\n",
      "Done with epoch number: 29\n",
      "Done with epoch number: 30\n",
      "Done with epoch number: 31\n",
      "Done with epoch number: 32\n",
      "Done with epoch number: 33\n",
      "Done with epoch number: 34\n",
      "Done with epoch number: 35\n",
      "Done with epoch number: 36\n",
      "Done with epoch number: 37\n",
      "Done with epoch number: 38\n",
      "Done with epoch number: 39\n",
      "Done with epoch number: 40\n",
      "Done with epoch number: 41\n",
      "Done with epoch number: 42\n",
      "Done with epoch number: 43\n",
      "Done with epoch number: 44\n",
      "Done with epoch number: 45\n",
      "Done with epoch number: 46\n",
      "Done with epoch number: 47\n",
      "Done with epoch number: 48\n",
      "Done with epoch number: 49\n",
      "Done with epoch number: 50\n",
      "Done with epoch number: 51\n",
      "Done with epoch number: 52\n",
      "Done with epoch number: 53\n",
      "Done with epoch number: 54\n",
      "Done with epoch number: 55\n",
      "Done with epoch number: 56\n",
      "Done with epoch number: 57\n",
      "Done with epoch number: 58\n",
      "Done with epoch number: 59\n",
      "Done with epoch number: 60\n",
      "Done with epoch number: 61\n",
      "Done with epoch number: 62\n",
      "Done with epoch number: 63\n",
      "Done with epoch number: 64\n",
      "Done with epoch number: 65\n",
      "Done with epoch number: 66\n",
      "Done with epoch number: 67\n",
      "Done with epoch number: 68\n",
      "Done with epoch number: 69\n",
      "Done with epoch number: 70\n",
      "Done with epoch number: 71\n",
      "Done with epoch number: 72\n",
      "Done with epoch number: 73\n",
      "Done with epoch number: 74\n",
      "Done with epoch number: 75\n",
      "Done with epoch number: 76\n",
      "Done with epoch number: 77\n",
      "Done with epoch number: 78\n",
      "Done with epoch number: 79\n",
      "Done with epoch number: 80\n",
      "Done with epoch number: 81\n",
      "Done with epoch number: 82\n",
      "Done with epoch number: 83\n",
      "Done with epoch number: 84\n",
      "Done with epoch number: 85\n",
      "Done with epoch number: 86\n",
      "Done with epoch number: 87\n",
      "Done with epoch number: 88\n",
      "Done with epoch number: 89\n",
      "Done with epoch number: 90\n",
      "Done with epoch number: 91\n",
      "Done with epoch number: 92\n",
      "Done with epoch number: 93\n",
      "Done with epoch number: 94\n",
      "Done with epoch number: 95\n",
      "Done with epoch number: 96\n",
      "Done with epoch number: 97\n",
      "Done with epoch number: 98\n",
      "Done with epoch number: 99\n",
      "Done with epoch number: 100\n",
      "loss 0.273 on cpu\n"
     ]
    }
   ],
   "source": [
    "lr = 0.005\n",
    "num_epochs = 100\n",
    "train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28da64fe-8d8b-41d9-a297-ec574fd77bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,device, save_attention_weights=False):\n",
    "    # Set `net` to eval mode for inference\n",
    "    net.eval()\n",
    "    src_tokens = src_vocab[src_sentence.lower().split()] + [src_vocab['<eos>']]\n",
    "    src_tokens = [src_vocab[token] if token in src_vocab.token_to_idx else src_vocab['<unk>'] for token in src_sentence.lower().split()]\n",
    "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
    "    src_tokens = truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    # Unsqueeze adds another dimension that works as the the batch axis here\n",
    "    enc_X = torch.unsqueeze(torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    # Add the batch axis to the decoder now\n",
    "    dec_X = torch.unsqueeze(torch.tensor([tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    for _ in range(num_steps):\n",
    "        Y = net.decoder(dec_X, dec_state)[0]\n",
    "        # We use the token with the highest prediction likelihood as the input\n",
    "        # of the decoder at the next time step\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # Save attention weights\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights)\n",
    "            # Once the end-of-sequence token is predicted, the generation of the output sequence is complete\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "                break\n",
    "        output_seq.append(pred)\n",
    "    if len(output_seq)<2:\n",
    "        \n",
    "        if len(output_seq)==1: \n",
    "            return ''.join(tgt_vocab.to_tokens(output_seq[0])), attention_weight_seq   \n",
    "        else:\n",
    "            \n",
    "            return \"No output!\", attention_weight_seq\n",
    "    else:\n",
    "        return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5f7b445-da55-4302-bee5-13b7a552cc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED : great product ::=>\tACTUAL : amazingly delicious\n",
      "PREDICTED : great ::=>\tACTUAL : excellent\n",
      "PREDICTED : best hot cocoa ::=>\tACTUAL : love\n",
      "PREDICTED : great coffee ::=>\tACTUAL : great coffee\n",
      "PREDICTED : great coffee ::=>\tACTUAL : wonderful cups\n",
      "PREDICTED : great ::=>\tACTUAL : tasty\n",
      "PREDICTED : great product ::=>\tACTUAL : delicious\n",
      "PREDICTED : great ::=>\tACTUAL : delicious\n",
      "PREDICTED : great product ::=>\tACTUAL : olive juice\n",
      "PREDICTED : great product ::=>\tACTUAL : outstanding\n"
     ]
    }
   ],
   "source": [
    "sample = x_test[:10]\n",
    "actual = y_test[:10]\n",
    "for s, a in zip(sample, actual):\n",
    "    pred_sum, _ = predict_seq2seq(net, s, src_vocab, tgt_vocab, 10, device)\n",
    "    print(\"PREDICTED : {} ::=>\".format(pred_sum), end='\\t')\n",
    "    print(\"ACTUAL : {}\".format(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b62f61e4-c685-4337-9790-f58814624f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6273    sooo happy amazon sells one grocery store neig...\n",
      "5739    running vanilla would made using vanilla beans...\n",
      "3238    husband love hot chocolate auto shipment every...\n",
      "6201    buying coffee least years love smooth taste hi...\n",
      "8325    wolfgang puck coffee chef reseerve columbian d...\n",
      "5105    eat chips often favorites flavor texture tradi...\n",
      "5440    jelly tastes heavenly wont find anything good ...\n",
      "431     done side side comparison chips still taste li...\n",
      "1954    purchased item cheaper olive juices looking go...\n",
      "3555    recommended friend saw gym seen times fatted c...\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8877de30-a726-4cb4-ad0e-66dc533fc87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommended friend saw gym seen times fatted calf napa didnt really pay attention never buy beans bag grocery store huge difference taste quality sent son aspiring chef gourmet foodie box beans bag well\n"
     ]
    }
   ],
   "source": [
    "first_sample = sample.iloc[9]\n",
    "print(first_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24234704",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"transformer_summarizer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c1eaa6-252d-401e-9bdf-1cdcf1830473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_product_reviews(product_id, df, net, src_vocab, tgt_vocab, num_steps, device):\n",
    "    # Filter reviews for the given product ID\n",
    "    reviews = df[df['ProductId'] == product_id]['Text'].dropna().tolist()\n",
    "    if not reviews:\n",
    "        return \"No reviews found for this product ID.\"\n",
    "    \n",
    "    # Clean and concatenate all reviews into one string\n",
    "    cleaned_reviews = [text_cleaner(review) for review in reviews]\n",
    "    full_text = ' '.join(cleaned_reviews)\n",
    "    \n",
    "    # Summarize\n",
    "    summary, _ = predict_seq2seq(net, full_text, src_vocab, tgt_vocab, num_steps, device)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27fdef25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good coffee brews hot\n"
     ]
    }
   ],
   "source": [
    "product_id = \"B003VXFK44\"\n",
    "print(summarize_product_reviews(product_id, df, net, src_vocab, tgt_vocab, max_len_text, device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
